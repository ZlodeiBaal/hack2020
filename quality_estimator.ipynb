{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from metrics import dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    \"dice_coefficient\": {\"2d\": dice_coefficient, \"3d\": dice_coefficient}\n",
    "}\n",
    "\n",
    "unary_metrics_dict = {\n",
    "    \"area\": {\"2d\": lambda x: (x > 0).sum(), \"3d\": lambda x: (x > 0).sum()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQualityEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Base Estimator for segmentation quality assessment\"\"\"\n",
    "\n",
    "    def __init__(self, metrics=[\"dice_coefficient\"], unary_metrics=[\"area\"], meta_clf=LGBMClassifier()):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics: list of strings: metrics to be computed on pairs of preds and gt\n",
    "            unary_metrics: list of string: metrics to be computed on preds directly\n",
    "        \n",
    "        TODO: params??\n",
    "        \"\"\"\n",
    "        self.meta_clf = meta_clf\n",
    "        self.metrics = list(filter(lambda _: _ in metrics_dict, metrics))\n",
    "        self.unary_metrics = list(filter(lambda _: _ in unary_metrics_dict, unary_metrics))\n",
    "        \n",
    "        self.data_type = None\n",
    "        self.X_metrics = None\n",
    "\n",
    "    def fit(self, X, Xy=None, y=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(X) == len(Xy) == len(y)\n",
    "        # get the dimensionality of the data\n",
    "        self.data_type = self._check_data_type(X)\n",
    "        # compute all the metrics on the pairs from X (predictions) and Xy (gt)\n",
    "        self.X_metrics = self._compute_metrics(X, Xy)\n",
    "        # fit meta-classifier to metrics and human-made labels\n",
    "        self.meta_clf.fit(self.X_metrics, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, Xy):\n",
    "        \n",
    "        y_prob = self.meta_clf.predict_proba(X)\n",
    "        \n",
    "        return (y_prob > 0.5).astype(float)\n",
    "    \n",
    "    def predict_proba(self, X, Xy):\n",
    "        \n",
    "        X_metrics = self._compute_metrics(X, Xy)\n",
    "        \n",
    "        y_pred = self.meta_clf.predict_proba(X_metrics)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def _compute_metrics(self, X, Xy):\n",
    "        \n",
    "        def _metrics(x, xy):\n",
    "            metrics_computed = dict()\n",
    "            for metric_ in self.metrics:\n",
    "                metrics_computed[metric_] = metrics_dict[metric_][self.data_type](x, xy)\n",
    "            return metrics_computed\n",
    "        \n",
    "        def _unary_metrics(x):\n",
    "            unary_metrics_computed = dict()\n",
    "            for metric_ in self.unary_metrics:\n",
    "                unary_metrics_computed[metric_] = unary_metrics_dict[metric_][self.data_type](x)\n",
    "            \n",
    "            return unary_metrics_computed\n",
    "        \n",
    "        metrics_computed = []\n",
    "        \n",
    "        for x_, xy_ in zip(X, Xy):\n",
    "            metrics_temp_ = _metrics(x_, xy_)\n",
    "            metrics_temp_.update(_unary_metrics(x_))\n",
    "            metrics_computed.append(metrics_temp_)\n",
    "            \n",
    "        df_metrics_computed = pd.DataFrame(metrics_computed)\n",
    "        \n",
    "        return df_metrics_computed\n",
    "        \n",
    "    def _check_data_type(self, X):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "        # заглушка:\n",
    "        if len(X.shape) == 2:\n",
    "            return \"2d\"\n",
    "        elif X.shape[2] == 1:\n",
    "            return \"2d\"\n",
    "        else:\n",
    "            return \"3d\"\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        # counts number of values bigger than mean\n",
    "        return(sum(self.predict(X))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(0, 1 + 1, size=[10, 300, 300, 3])\n",
    "Xy = np.random.randint(0, 1 + 1, size=[10, 300, 300, 3])\n",
    "y = np.random.randint(0, 1 + 1, size=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000644068517781"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_coefficient(img_1, img_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_clf = BaseQualityEstimator( metrics=[\"dice_coefficient\"], unary_metrics=[\"area\"], meta_clf=LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseQualityEstimator()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_clf.fit(X, Xy, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_clf.predict_proba(X, Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict({1: 2})\n",
    "c = dict({2: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d6b92217a5ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src/'))\n",
    "    \n",
    "from dpipe_metrics import hausdorff_distance, surface_distances, dice_score, assd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    \"dice_coefficient\": {\"2d\": dice_score, \"3d\": dice_score},\n",
    "#     \"mae\": {\"2d\": lambda x, y: np.abs(x - y).mean(), \"3d\": lambda x, y: np.abs(x - y).mean()},\n",
    "#     \"mse\": {\"2d\": lambda x, y: ((x - y) ** 2).mean(), \"3d\": lambda x, y: ((x - y) ** 2).mean()},\n",
    "    \"hausdorff_distance\": {\"2d\": hausdorff_distance, \"3d\": hausdorff_distance},\n",
    "    \"surface_distances\": {\"2d\": surface_distances, \"3d\": surface_distances},\n",
    "    \"assd\": {\"2d\": assd, \"3d\": assd}\n",
    "}\n",
    "\n",
    "unary_metrics_dict = {\n",
    "    \"area\": {\"2d\": lambda x: (x > 0).sum(), \"3d\": lambda x: (x > 0).sum()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQualityEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Base Estimator for segmentation quality assessment\"\"\"\n",
    "\n",
    "    def __init__(self, metrics=[\"dice_coefficient\"], unary_metrics=[\"area\"], meta_clf=LGBMClassifier()):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics: list of strings: metrics to be computed on pairs of preds and gt\n",
    "            unary_metrics: list of string: metrics to be computed on preds directly\n",
    "        \n",
    "        TODO: params??\n",
    "        \"\"\"\n",
    "        self.meta_clf = meta_clf\n",
    "        self.metrics = list(filter(lambda _: _ in metrics_dict, metrics))\n",
    "        self.unary_metrics = list(filter(lambda _: _ in unary_metrics_dict, unary_metrics))\n",
    "        \n",
    "        self.data_type = \"3d\"\n",
    "        self.X_metrics = None\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Xy=None, y=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(X) == len(Xy) == len(y)\n",
    "        # get the dimensionality of the data\n",
    "#         self.data_type = self._check_data_type(X)\n",
    "        # compute all the metrics on the pairs from X (predictions) and Xy (gt)\n",
    "        self.X_metrics = self._compute_metrics(X, Xy)\n",
    "        # fit meta-classifier to metrics and human-made labels\n",
    "        self.meta_clf.fit(self.X_metrics, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, Xy):\n",
    "        \n",
    "        X_metrics = self._compute_metrics(X, Xy)\n",
    "        \n",
    "        y_pred = self.meta_clf.predict(X_metrics)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X, Xy):\n",
    "        \n",
    "        X_metrics = self._compute_metrics(X, Xy)\n",
    "        \n",
    "        y_pred = self.meta_clf.predict_proba(X_metrics)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def _compute_metrics(self, X, Xy):\n",
    "        \n",
    "        def _metrics(x, xy):\n",
    "            metrics_computed = dict()\n",
    "            for metric_ in self.metrics:\n",
    "                metrics_computed[metric_] = metrics_dict[metric_][self.data_type](x, xy)\n",
    "            return metrics_computed\n",
    "        \n",
    "        def _unary_metrics(x):\n",
    "            unary_metrics_computed = dict()\n",
    "            for metric_ in self.unary_metrics:\n",
    "                unary_metrics_computed[metric_] = unary_metrics_dict[metric_][self.data_type](x)\n",
    "            \n",
    "            return unary_metrics_computed\n",
    "        \n",
    "        metrics_computed = []\n",
    "        \n",
    "        for x_, xy_ in zip(X, Xy):\n",
    "            metrics_temp_ = _metrics(x_, xy_)\n",
    "            metrics_temp_.update(_unary_metrics(x_))\n",
    "            metrics_computed.append(metrics_temp_)\n",
    "            \n",
    "        df_metrics_computed = pd.DataFrame(metrics_computed)\n",
    "        \n",
    "        return df_metrics_computed\n",
    "        \n",
    "    def _check_data_type(self, X):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "        # заглушка:\n",
    "        if len(X.shape) == 2:\n",
    "            return \"2d\"\n",
    "        elif X.shape[2] == 1:\n",
    "            return \"2d\"\n",
    "        else:\n",
    "            return \"3d\"\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        # counts number of values bigger than mean\n",
    "        return(sum(self.predict(X))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.read_csv(\"../data/OpenPart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([plt.imread(_) for _ in sorted(glob.glob(\"../data/sample_1/*\"))]).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([plt.imread(_) for _ in sorted(glob.glob(\"../data/sample_1/*\"))]).astype(bool)\n",
    "X_ids = [_.split(\"/\")[-1] for _ in sorted(glob.glob(\"../data/sample_1/*\"))]\n",
    "Xy = np.array([plt.imread(_) for _ in glob.glob(\"../data/after/*\")]).astype(bool)\n",
    "Xy_ids = [_.split(\"/\")[-1] for _ in sorted(glob.glob(\"../data/after/*\"))]\n",
    "\n",
    "y = y_df.sort_values(by=\"Case\")[\"Sample 1\"].values\n",
    "y_ids = y_df.sort_values(by=\"Case\")[\"Case\"].values\n",
    "\n",
    "X_not_ids = [X_ids[i] for i in range(len(X)) if X_ids[i] not in y_ids]\n",
    "\n",
    "X = np.array([X[i] for i in range(len(X)) if X_ids[i] in y_ids])\n",
    "Xy = np.array([Xy[i] for i in range(len(Xy)) if Xy_ids[i] in y_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, Xy_train, Xy_eval, y_train, y_eval = train_test_split(X, Xy, y, test_size=0.2, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_clf = BaseQualityEstimator(metrics=[\"dice_coefficient\", \n",
    "                                      \"mae\", \"mse\", \n",
    "                                      \"hausdorff_distance\", \n",
    "#                                       \"surface_distances\",\n",
    "                                      \"assd\"\n",
    "                                     ], \n",
    "                             unary_metrics=[\"area\"], meta_clf=LGBMClassifier(max_depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 1024, 1024), (48, 1024, 1024), (48,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Xy_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_clf.X_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseQualityEstimator(meta_clf=LGBMClassifier(max_depth=3),\n",
       "                     metrics=['dice_coefficient', 'hausdorff_distance', 'assd'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_clf.fit(X_train, Xy_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = q_clf.predict(X_eval, Xy_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = np.array([3 for i in range(len(y_eval))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 4, 3, 5, 4, 5, 2, 3, 4, 3, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 1, 1, 4, 5, 4, 3, 3, 2, 5, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1666666666666667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_eval, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.25"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
